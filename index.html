<!DOCTYPE HTML>
<!--
	Photon by HTML5 UPhould we
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>NeurIPS2020 pre-registration workshop</title>
		<meta charset="utf-8" />
		<link rel="canonical" href="https://preregister.science"/>
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<!-- Open Graph metadata (used when sharing on social media) -->
		<meta property="og:type" content="website" />
		<meta property="og:title" content="NeurIPS2020 pre-registration workshop" />
		<meta property="og:description" content="Testing whether pre-registration can help fix our peer review system." />
		<meta property="og:url" content="https://preregister.science" />
		<meta property="og:image" content="https://preregister.science/images/header.jpg" />

	</head>
	<body class="is-preload">
		<!-- Header -->
		<section id="header">
			<div class="inner">
				<span class="icon major style0 fa fa-area-chart"></span>
				<h2 style="background-color: rgba(0,0,0,0.1)">The <strong style="color:#FCCA46;">pre-registration experiment</strong>: <br>an alternative publication model for machine learning research</h2>
				<h3 style="background-color: rgba(0,0,0,0.1)">This workshop took place at <a href="https://neurips.cc/" target="_blank">NeurIPS 2020</a>, on December 11</h3>
                <h3 style="background-color: rgba(0,0,0,0.1)">Links to the content of the workshop can be found here: <br><a href="https://neurips.cc/virtual/2020/protected/workshop_16158.html">https://neurips.cc/virtual/2020/protected/workshop_16158.html</a>.</h3>
                <h3 style="background-color:red">The PMLR submission deadline has been extended to 7th May 2021, 23:59, Anwyhere on Earth timezone</h3>
			</div>
		</section>
		<!-- ------------------------------------------------------------------------------------------------------->
		<section class="main style1 special">
			<div class="container">
				<header class="major">
					<h2>Speakers</h2>
				</header>
				<!-- <p>More to be announced</p> -->
				<div class="row gtr-150">
					
					<div class="off-1 col-2 off-0-medium col-4-medium">
                        (<span class="icon fa-video-camera"> <a href="https://slideslive.com/38938274/incentives-for-researchers">Video</a>)
						<span class="image fit"><img src="images/bengio.jpg" alt="Yoshua Bengio"></span>
						<h4><a href="https://yoshuabengio.org/">Yoshua Bengio</a></h4>
						<p>MILA</p>
					</div>
					
					<div class="col-2 col-4-medium">
                        (<span class="icon fa-video-camera"> <a href="https://slideslive.com/38938275/can-preregistration-lead-to-better-reproducibility-in-ml-research">Video</a>, <a href="slides/jpineau-NeurIPSwsPreregistration-dec20.pdf">Slides</a>)
						<span class="image fit"><img src="images/pineau.jpg" alt="Joelle Pineau"></span>
						<h4><a href="https://www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a></h4>
						<p>FAIR, MILA, McGill</p>
					</div>

					<div class="col-2 col-4-medium">
                        (<span class="icon fa-video-camera"> <a href="https://slideslive.com/38938273/where-is-machine-learning-going">Video</a>, <a href="slides/francis.pdf">Slides</a>)
						<span class="image fit"><img src="images/bach.jpg" alt="Francis Bach"></span>
						<h4><a href="https://www.di.ens.fr/~fbach/">Francis Bach</a></h4>
						<p>INRIA</p>
					</div>

					<div class="col-2 col-4-medium">
                        (Live)
						<span class="image fit"><img src="images/forde.jpg" alt="Jessica Zosa Forde"></span>
						<h4><a href="https://jzf2101.github.io/">Jessica Zosa Forde</a></h4>
						<p>Brown University</p>
					</div>

					<div class="col-2 col-4-medium">
                        (<span class="icon fa-video-camera"> <a href="https://slideslive.com/38938270/the-turing-way-transparent-research-through-the-scientific-lifecycle">Video</a>, <a href="slides/Whitaker_TuringWay_NeurIPSPreregistration_November2020.pdf">Slides</a>)
						<span class="image fit"><img src="images/whitaker.jpg" alt="Kirstie Whitaker"></span>
						<h3><a href="https://www.turing.ac.uk/people/researchers/kirstie-whitaker">Kirstie Whitaker</a></h3>
						<p>Alan Turing Institute, Cambridge University</p>
					</div>

				</div>

            <h2>Schedule</h2>
            <div id="calendar-container"></div>
            <!--<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jstimezonedetect/1.0.4/jstz.min.js"></script>-->
            <script type="text/javascript">
                 // var timezone = encodeURIComponent(jstz.determine().name()); 
                 var timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                 var pref = '<iframe src="https://calendar.google.com/calendar/embed?mode=agenda&dates=20201211/20201212&src=NzBhOG01dWs0N2lhNmhicmYzcW5paXVxZzRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&ctz=';
                 var suff = '" style=" border-width:0 " width="800" height="600" frameborder="0" scrolling="no"></iframe>';
                 var iframe_html = pref + timezone + suff;
                 document.getElementById('calendar-container').innerHTML = iframe_html;
            </script>

			<!--<iframe src="https://calendar.google.com/calendar/embed?height=600&amp;wkst=1&amp;bgcolor=%23ffffff&amp;ctz=America%2FToronto&amp;src=NzBhOG01dWs0N2lhNmhicmYzcW5paXVxZzRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;color=%239E69AF&amp;mode=AGENDA" style="border-width:0" width="800" height="600" frameborder="0" scrolling="no"></iframe>-->
			</div>
		</section>		

		<!-- ------------------------------------------------------------------------------------------------------->
		<section class="main style2 special">
			<div class="container">
				<header class="major">
					<h2>Pre-registration in a nutshell</h2>
				</header>
				<div class="container">
          <h3>Separate the generation and confirmation of hypotheses:</h3>
					<h4><span class="icon fa-lightbulb-o"></span>&nbsp;&nbsp;&nbsp;Come up with an exciting research question&nbsp;&nbsp;&nbsp;<span class="icon fa-lightbulb-o"></span></h4>
					<h4><span class="icon fa-pencil"></span>&nbsp;&nbsp;&nbsp;Write a paper proposal without confirmatory experiments&nbsp;&nbsp;&nbsp;<span class="icon fa-pencil"></span></h4>
					<h4><span class="icon fa-flask"></span>&nbsp;&nbsp;&nbsp;After the paper is accepted, run the experiments and report your results&nbsp;&nbsp;&nbsp;<span class="icon fa-flask"></span></h4>
				</div>
			</div>
		</section>

		<!-- ------------------------------------------------------------------------------------------------------->
		<section class="main style1">
			<div class="container">
				<div class="row gtr-0">
					<div class="col-3">
						<ul class="major-icons">
							<li><span class="icon major fa-balance-scale"></span></li>
						</ul>
					</div>
					<div class="col-9">
						<header class="major">
						<h2>What does <strong style="color:#FCCA46;">science</strong> get?</h2>
						</header>
						<ul>
							<li>A healthy mix of positive and negative results</li>
							<li>Reasonable ideas that don’t work still get published, avoiding wasteful replications</li>	
							<li>Papers are evaluated on the basis of scientific interest, not whether they achieve the best results</li>
						</ul>
					</div>
				</div>
				<div class="row gtr-0">
					<div class="col-3">
						<ul class="major-icons">
							<li><span class="icon major fa-rocket"></span></li>
						</ul>
					</div>
					<div class="col-9">
						<header class="major">
						<h2>What do <strong style="color:#FCCA46;">you</strong> get?</h2>
						</header>
						<ul>
							<li>It's easier to plan your research: get feedback before investing in lengthy experiments</li>
              <!--<li>Reviews that are not biased towards authors with an identical idea, but greater computational resources who have greater ability to cherry-pick.</li>-->
							<li>Your research is stronger: results have increased credibility</li>
							<!--<li>Your paper gets reviewed on the strength of your arguments and ideas, not numbers</li>-->
							<li>Convince people that they will learn something even if the result is negative</li>
							<!--<li>Unexpected findings can still be reported, but are not confused with hypothesis testing</li>-->
						</ul>
					</div>
				</div>
			</div>
		</section>
		<!-- ------------------------------------------------------------------------------------------------------->
		<section class="main style2">
			<div class="container">
				<header class="major special">
					<h2>Call for Papers</h2>
				</header>
                <del>
				<div class="container">
					<p>
						<strong>What is pre-registration and how does it improve peer review?</strong>
						Benchmarks on popular datasets have played a key role in the considerable measurable progress that machine learning has made in the last few years.
						But reviewers can be tempted to prioritize incremental improvements in benchmarks to the detriment of other scientific criteria, destroying many good ideas in their infancy.
						Authors can also feel obligated to make orthogonal improvements in order to “beat the state-of-the-art”, making the main contribution hard to assess.
					</p>
					<p>
						Pre-registration changes the incentives by <i>reviewing and accepting a paper before experiments are conducted</i>.
						The emphasis of peer-review will be on whether the experiment plan can adequately prove or disprove one (or more) hypotheses.
						Some results will be negative, and this is welcomed.
						This way, good ideas that do not work will get published, instead of filed away and wastefully replicated many times by different groups.
						Finally, the clear separation between hypothesizing and confirmation (absent in the current review model) will raise the statistical significance of the results.
					</p>
					<p>
						We are inviting submissions on the broad range of topics covered at <a href="https://neurips.cc/Conferences/2020/PaperInformation/SubjectAreas">NeurIPS</a>! The <a href="author-kit/neurips2020_preregistration_template.zip">paper template</a> is structured like a <a href="author-kit/tutorial.pdf">mini-tutorial</a> on the pre-registration process to get you started quickly.
						Pre-registered papers will be published at the workshop. Authors will then have the opportunity to submit the results paper to the Proceedings of Machine Learning Research (<a href="http://proceedings.mlr.press/">PMLR</a>), a sister publication to the Journal of Machine Learning Research (<a href="https://www.jmlr.org/">JMLR</a>). The review process for this second stage will aim to ensure that the authors have performed a good-faith attempt to complete the experiments described in their proposal paper.
					</p>
				</div>
            </del>
				<div class="container">
                    Please note that the workshop and pre-registration phase has now taken place. <strong style="color:orange">The results phase is now active</strong> (see below).
				</div>
			</div>
		</section>
		<!-- ------------------------------------------------------------------------------------------------------->
		<section class="main style1">
			<div class="container">
				<header class="major">
					<h2>Important info and dates</h2>
				</header>
				<div>
					<p>The review cycle for a pre-registered study consists of two stages: the <strong>proposal paper</strong> and the <strong>results paper</strong>.
					These stages reflect the exploratory (hypothesis generation) and confirmatory (hypothesis testing) phases of research.</p>

					<h3>Proposal paper</h3>
                    <del>
					<ul>
						<li>Read our <a href="author-kit/neurips2020_preregistration_template.zip"><strong>mini-tutorial/template</strong></a> (<a href="author-kit/tutorial.pdf">PDF</a>) &mdash; it serves as a paper template, describes the submission process and the intended spirit of a pre-registration.</li>
						<li><strong>Submit</strong> your paper anonymously to <a href="https://cmt3.research.microsoft.com/NeurIPSWorkshopPrereg2020">CMT</a>.
							Differently from traditional submissions, the experimental section must only contain a description of experiments and protocol, and what conclusions can be drawn in different cases, without the results themselves.
                                                        The pre-registration proposal should use the <a href="author-kit/neurips2020_preregistration_template.zip">paper template</a>. 
							We recommend <strong>4 pages</strong>, but we allow up to 5 pages (excluding references) for the pre-registration proposal. Note that, for some venues, only papers up to 4 pages (without references) are not considered as 'prior submission', see e.g. <a href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines">CVPR</a>. For others, e.g. <a href="https://neurips.cc/Conferences/2020/PaperInformation/NeurIPS-FAQ">NeurIPS</a>, non-archival workshops like ours do not count as dual submissions. <strong>The deadline for submissions is <strong style="color:red">October <strike>7th</strike> 9th (midnight Anywhere on Earth: GMT + 12).</strong></strong>
						<li>Besides quality and potential impact of the idea, reviewers will also assess: (1) Are the experiments appropriate for validating the core hypothesis of the work?
						(2) Is the experimental protocol description sufficient to allow reproduction of the experiments?
						You will then have a <strong>rebuttal period (until October 22nd)</strong> to address the comments of the reviewers, by writing a short response.
						<li><strong>Decisions will be sent to authors by October 30th</strong>.</li>
						<li><strong>On the day of the workop (December 11th, <strong style="color:red">remote</strong>, from 9:20AM New York time))</strong>, authors will present their proposals and (optionally) their preliminary results.</li>
					</ul>
                    </del>

					<h3>Results paper</h3>
					<ul>
					<li>Authors carry out the experimental protocols proposed in their accepted proposal papers.
					<li>The results will be presented in second document known as the results paper.
<!--						<strong>The results paper consists of just the &ldquo;experimental results&rdquo; and &ldquo;conclusions&rdquo; sections of standard publications</strong>.-->
					This will be appended to the proposal paper to form the complete document.
					The deadline for the results paper is going to be in <del>April, 2021 (tentative)</del> <strong style="color:red">The deadline has been extended to Friday 7th May, 23:59, Anywhere on Earth time zone.</strong></li>
					<li>We will then support and encourage the final results to be published at the <a href="http://proceedings.mlr.press/">PMLR</a> in combination with the pre-registered paper.</li>
					<li><del>In case of interest, we will also organise a second virtual meeting at the end of April 2021 to discuss the experimental results and the lesssons learned.</del> (the next steps are currently under consideration)</li>
					</ul>
				</div>
			</div>
		</section>

        <!-- ------------------------------------------------------------------------------------------------------->
        <section class="main style1">
        <div class="container">

        <div class="col-10-medium off-1-medium col-8-large off-2-large col-8-xlarge off-2-xlarge col-8-max off-2-max">
            <!--<p> </p>-->
            <header class="major special">
                <h2>Accepted Proposals</h2>
            </header>

            <div class="table-wrapper">
                <p align="center">
                    Playlist of all 1-minute preview videos<br>
                    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/videoseries?list=PLB2hvs7du3_cSo0SHJvGwiivIiedoaLDG" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </p>
                <table>
                    <tbody>
                        <tr>
                            <td>
                                5
                            </td>
                            <td>
                                Kexue Fu, Xiaoyuan Luo, Manning Wang
                            </td>
                            <td>
                                <em>
                                Point Cloud Overlapping Region Co-Segmentation Network
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/5_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/j0f1ZIhosSE">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/5_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                7
                            </td>
                            <td>
                                Udo Schlegel, Daniela Oelke, Daniel Keim, Mennatallah El-Assady
                            </td>
                            <td>
                                <em>
                                An Empirical Study of Explainable AI Techniques on Deep Learning Models For Time Series Tasks
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/7_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/4ok2O7L1y_w">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/7_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                17
                            </td>
                            <td>
                                Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, Vineeth N Balasubramanian
                            </td>
                            <td>
                                <em>
                                On Initial Pools for Deep Active Learning
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/17_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/HR3BPJ46Lwk">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/17_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                19
                            </td>
                            <td>
                                Liu Yuezhang, Bo Li, Qifeng Chen
                            </td>
                            <td>
                                <em>
                                Evaluating Adversarial Robustness in Simulated Cerebellum
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/19_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/1wAex6DuExU">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/19_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                21
                            </td>
                            <td>
                                XueHao Gao, Yang Yang, Shaoyi Du
                            </td>
                            <td>
                                <em>
                                Contrastive Self-Supervised Learning for Skeleton Action Recognition
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/21_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/RJeuW4MXKDU">Video</a>
                                <a href="https://slideslive.com/38938269/contrastive-selfsupervised-learning-for-skeleton-action-recognition">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/21_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                26
                            </td>
                            <td>
                                Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Prem Natarajan
                            </td>
                            <td>
                                <em>
                                Keypoints-aware Object Detection
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/26_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/iis3h5ri0do">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/26_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                27
                            </td>
                            <td>
                                Cade Gordon, Natalie Parde
                            </td>
                            <td>
                                <em>
                                Latent Neural Differential Equations for Video Generation
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/27_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/iden4Lab0q4">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/27_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                28
                            </td>
                            <td>
                                Robert Vandermeulen, Rene Saitenmacher, Alexander Ritchie
                            </td>
                            <td>
                                <em>
                                A Proposal for Supervised Density Estimation
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/28_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/MCf9Xg2HJfA">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/28_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                31
                            </td>
                            <td>
                                Eimear O' Sullivan, Stefanos Zafeiriou
                            </td>
                            <td>
                                <em>
                                PCA Retargeting: Encoding Linear Shape Models as Convolutional Mesh Autoencoders
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/31_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/7DUqITmtqJ8">Video</a>
                                <a href="https://slideslive.com/38941993/pca-retargeting-encoding-linear-shape-models-as-convolutional-mesh-autoencoders">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/31_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                33
                            </td>
                            <td>
                                Rasmus Palm, Elias Najarro, Sebastian Risi
                            </td>
                            <td>
                                <em>
                                Testing the Genomic Bottleneck Hypothesis in Hebbian Meta-Learning
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/33_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/0utTI1KiEfs">Video</a>
                                <a href="https://slideslive.com/38941995/testing-the-genomic-bottleneck-hypothesis-in-hebbian-metalearning">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/33_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                36
                            </td>
                            <td>
                                Rodrigo Alves, Antoine Ledent, Renato Assunção, Marius Kloft
                            </td>
                            <td>
                                <em>
                                An Empirical Study of the Discreteness Prior in Low-Rank Matrix Completion
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/36_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/g2KQ2hp-Ifw">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/36_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                38
                            </td>
                            <td>
                                Elena Burceanu
                            </td>
                            <td>
                                <em>
                                SFTrack++: A Fast Learnable Spectral Segmentation Approach for Space-Time Consistent Tracking
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/38_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/VTaHNqRI-bU">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/38_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                39
                            </td>
                            <td>
                                Rishika Bhagwatkar, Khurshed Fitter, Saketh Bachu, Akshay Kulkarni, Shital Chiddarwar
                            </td>
                            <td>
                                <em>
                                Paying Attention to Video Generation
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/39_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/PhS5ns-5GZE">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/39_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                40
                            </td>
                            <td>
                                Chen Li, Xutan Peng, Hao Peng, Jianxin Li, Lihong Wang, Philip Yu,
                            </td>
                            <td>
                                <em>
                                TextSGCN: Document-Level Graph Topology Refinement for Text Classification
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/40_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/BdYiXyDnUd0">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/40_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                41
                            </td>
                            <td>
                                Chase Dowling, Ted Fujimoto, Nathan Hodas
                            </td>
                            <td>
                                <em>
                                Policy Convergence Under the Influence of Antagonistic Agents in Markov Games
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/41_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/HQp0UeVH2No">Video</a>
                                <a href="https://slideslive.com/38941997/policy-convergence-under-the-influence-of-antagonistic-agents-in-markov-games">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/41_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                42
                            </td>
                            <td>
                                Arnout Devos ,Yatin Dandi
                            </td>
                            <td>
                                <em>
                                Model-Agnostic Learning to Meta-Learn
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/42_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/1oFMz27zgs0">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/42_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                44
                            </td>
                            <td>
                                Carianne Martinez, Adam Brink, David Najera-Flores, D. Dane Quinn, Eleni Chatzi, Stephanie Forrest,
                            </td>
                            <td>
                                <em>
                                Confronting Domain Shift in Trained Neural Networks
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/44_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/2c66QVAqA6o">Video</a>
                                <a href="https://slideslive.com/38938272/confronting-domain-shift-in-trained-neural-networks">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/44_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                45
                            </td>
                            <td>
                                Joao Monteiro, Xavier Gibert, Jianqiao Feng, Vincent Dumoulin, Dar-Shyang Lee
                            </td>
                            <td>
                                <em>
                                Domain Conditional Predictors for Domain Adaptation
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/45_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/2TOnHAGTe8c">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/45_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                47
                            </td>
                            <td>
                                Tanner Bohn, Xinyu Yun, Charles Ling
                            </td>
                            <td>
                                <em>
                                Towards a Unified Lifelong Learning Framework
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/47_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/82-vbMigGxc">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/47_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                48
                            </td>
                            <td>
                                Hamid Eghbal-zadeh, Florian Henkel, Gerhard Widmer
                            </td>
                            <td>
                                <em>
                                Context-Adaptive Reinforcement Learning using Unsupervised Learning of Context Variables
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/48_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/gRgClu6yi3k">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/48_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                50
                            </td>
                            <td>
                                Aneesh Dahiya, Adrian Spurr, Otmar Hilliges
                            </td>
                            <td>
                                <em>
                                Exploring self-supervised learning techniques for hand pose estimation
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/50_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/QCA2a2UFu_w">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/50_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                55
                            </td>
                            <td>
                                Sebastian Stabinger, David Peer, Antonio Rodriguez-Sanchez
                            </td>
                            <td>
                                <em>
                                Training of Feedforward Networks Fails on a Simple Parity-Task
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/55_paper.pdf">PDF</a>
                                <a href="papers_20neurips/55_suppl.pdf">Supmat</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/zXa0JaqRxsQ">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/55_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                56
                            </td>
                            <td>
                                Pablo Barros, Ana Tanevska, Ozge Nilay Yalcin, Alessandra Sciutti
                            </td>
                            <td>
                                <em>
                                Incorporating Rivalry in Reinforcement Learning for a Competitive Game
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/56_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/JRwVRmmONig">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/56_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                57
                            </td>
                            <td>
                                Steffen Schneider, Shubham Krishna, Luisa Eck, Wieland Brendel, Mackenzie Mathis, Matthias Bethge
                            </td>
                            <td>
                                <em>
                                Generalized Invariant Risk Minimization: relating adaptation and invariant representation learning
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/57_paper.pdf">PDF</a>
                                <a href="papers_20neurips/57_suppl.pdf">Supmat</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/0B_kXJgvmj4">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/57_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                58
                            </td>
                            <td>
                                Alex Lewandowski
                            </td>
                            <td>
                                <em>
                                Generalization Across Space and Time in Reinforcement Learning
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/58_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/5wFArlLqsu8">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/58_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                59
                            </td>
                            <td>
                                Prabhu Pradhan, Ruchit Rawal, Gopi Kishan
                            </td>
                            <td>
                                <em>
                                Rendezvous between Robustness and Dataset Bias: An empirical study
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/59_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/1vjVhDUaieI">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/59_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                60
                            </td>
                            <td>
                                Miles Cranmer, Peter Melchior, Brian Nord
                            </td>
                            <td>
                                <em>
                                Unsupervised Resource Allocation with Graph Neural Networks
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/60_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/po5oB-P8Ppo">Video</a>
                                <a href="https://slideslive.com/38941999/unsupervised-resource-allocation-with-graph-neural-networks">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/60_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                62
                            </td>
                            <td>
                                Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva
                            </td>
                            <td>
                                <em>
                                Is High Quality Data All You Need?
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/62_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/wFMdMzpPRnU">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/62_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                67
                            </td>
                            <td>
                                Yi-Fan Li, Yang Gao, Yu Lin, Zhuoyi Wang, Latifur Khan
                            </td>
                            <td>
                                <em>
                                Time Series Forecasting Using a Unified Spatial-Temporal Graph Convolutional Network
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/67_paper.pdf">PDF</a>
                                <a href="papers_20neurips/67_suppl.pdf">Supmat</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/WoKitd4XSvs">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/67_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                69
                            </td>
                            <td>
                                Norman Tasfi, Eder Santana, Miriam Capretz
                            </td>
                            <td>
                                <em>
                                Policy Agnostic Successor Features
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/69_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/DgBJ4zYHBeE">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/69_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                71
                            </td>
                            <td>
                                Owen Lockwood, Mei Si
                            </td>
                            <td>
                                <em>
                                Playing Atari with Hybrid Quantum-Classical Reinforcement Learning
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/71_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/kavxptjgPgU">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/71_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                76
                            </td>
                            <td>
                                Ajinkya Mulay, Ayush Manish Agrawal, Tushar Semwal
                            </td>
                            <td>
                                <em>
                                FedPerf: A Practitioners' Guide to Performance of Federated Learning Algorithms
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/76_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/TxgLqE5nQlQ">Video</a>
                                <a href="https://slideslive.com/38942001/fedperf-a-practitioners-guide-to-performance-of-federated-learning-algorithms">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/76_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                77
                            </td>
                            <td>
                                Harshvardhan Sikka, Atharva Tendle, Amr Kayid
                            </td>
                            <td>
                                <em>
                                Multimodal Modular Meta-Learning
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/77_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/zrpNrogpwac">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/77_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                79
                            </td>
                            <td>
                                Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon
                            </td>
                            <td>
                                <em>
                                Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/79_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/wDflHW9JI4Q">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/79_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                81
                            </td>
                            <td>
                                Ruizhe Li, Xutan Peng, Chenghua Lin, Frank Guerin, Wenge Rong
                            </td>
                            <td>
                                <em>
                                On the low-density latent regions of VAE-based language models
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/81_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/oRccOg9PTQ0">Video</a>
                                <a href="https://slideslive.com/38942002/on-the-lowdensity-latent-regions-of-vaebased-language-models">Oral</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/81_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                82
                            </td>
                            <td>
                                Pratyush Kumar, Aishwarya Praveen Das, Debayan Gupta
                            </td>
                            <td>
                                <em>
                                Differential Euler: Designing a Neural Network approximator to solve the Chaotic Three Body Problem
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/82_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/H-U2DHyyH6Y">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/82_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                83
                            </td>
                            <td>
                                Jiaqi Fan, Junxin Huang, Xiaochuan Yu, Chao He
                            </td>
                            <td>
                                <em>
                                Data Subset Selection for Object Detection
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/83_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/h6iA2uW2uNA">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/83_poster.png">Poster</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                85
                            </td>
                            <td>
                                Meenakshi Sarkar, Debasish Ghose
                            </td>
                            <td>
                                <em>
                                Decomposing camera and object motion for an improved Video Sequence Prediction
                                </em>
                            </td>
                            <td>
                                <a href="papers_20neurips/85_paper.pdf">PDF</a>
                            </td>
                            <td>
                                <a href="https://youtu.be/DMe5GsVylXk">Video</a>
                            </td>
                            <td>
                                <a href="posters_20neurips/85_poster.png">Poster</a>
                            </td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        </div>
        </section>
		<!-- ------------------------------------------------------------------------------------------------------->
		<section class="main style2" id="instructions">
			<div class="container">
				<header class="major">
					<h2>FAQs</h2>
				</header>
				<ul class="spacious">
					<li><strong>Don't we need a positive publication bias? After all, there many more ideas that don't work than ones that do. Why is it useful to allow negative results?</strong><br>
						There are several benefits to publishing negative results.  If an idea is well-motivated and intuitively appealing, it may be needlessly repeated by multiple groups who replicate the negative outcome, but do not have a venue for sharing this knowledge with the community (see the CVPR 2017 workshop on <a href="http://negative.vision/">negative results</a> for a more detailed discussion of the benefits of publishing negative outcomes).
					</li>
					<li><strong>Doesn't prior work on existing benchmarks weaken my confirmatory experiments?</strong><br>
						Yes.  Each prior result reported on a dataset leaks information that reduces its statistical utility (we are strongly in favour of limited-evaluation benchmarks for this reason).  Unfortunately, from a pragmatic perspective, it is infeasible to expect every computer vision researcher to collect a new dataset for each hypothesis they wish to evaluate, so we must strike a reasonable balance here.
					</li>
					<li><strong>Is it OK to make changes to the preregistered experimental protocol?</strong><br>
						Although you should endeavour to follow the proposed experimental protocol as closely as possible, you may find that it is necessary to make small changes or refinements.  These changes should be carefully documented when reporting the experimental results: it is important to make clear which protocols have been modified after observing the evidence.
					</li>
                    <li><strong>Can the author list change from the proposal to the results paper?</strong><br>
                        New author(s) can be added for their contribution at the hypothesis testing phase, e.g., running the experiments.
                    </li>
					<li><strong>What prevents authors from secretly running experiments, and submitting a proposal omitting them?</strong><br>
						We cannot fully prevent unethical practices, but this is a risky strategy. The reviewers assume that no experiments were ran yet, so they can request significant changes to the protocol. This means that there is a significant chance those results cannot be used.
					</li>
					<li><strong>How does exploratory data analysis fit into this model?</strong><br>
						Exploratory analysis can come in multiple forms including:
						(1) Small scale experiments (typically on toy data);
						(2) Results listed in prior work.
						Both should be reported in the proposal paper as part of the justification for your idea. Neither should be considered by the reader of your paper as providing <em>confirmatory evidence</em> in support of your hypothesis (the goal of preregistration is to make this distinction explicit). By contrast, the confirmatory experimental protocol which you propose should seek to rigorously evaluate your hypothesis and must be performed on different data to your own exploratory experiments. However, for practical reasons, it may use datasets that have also been previously used in the literature (further discussion below).
					</li>
					<li><strong>What's the rationale for changing the review model?</strong><br>
						&ldquo;Preregistration separates hypothesis-generating  (exploratory) from hypothesis-testing (confirmatory) research. Both are important. But the same data cannot be used to generate and test a hypothesis, which can happen unintentionally and reduces the credibility of your results. Addressing this problem through planning improves the quality and transparency of your research, helping others who may wish to build on it.&rdquo; (source: <a href="https://cos.io/prereg/">cos.io</a>)
					</li>
					<li><strong>I have read the tutorial. Can you point me to a concrete example of a ML paper written with this spirit in mind?</strong><br>
						<a href="https://preregister.vision/papers/an_empirical_study_of_the_relation_between_network_architecture_and_complexity.pdf"><i>An empirical study of the relation between network architecture and complexity</i></a>, by Emir Konuk & Kevin Smith, won the best paper award at the pilot we run at ICCV 2019.
					</li>
					<li><strong>Where is the link to the pilot workshop you did at ICCV 2019?</strong><br>
						The previous edition of the workshop can be found <a href="https://preregister.vision">here</a>.
					</li>
					<li><strong>Does my proposal paper require the "Broader Impacts" section introduced by NeurIPS last year?</strong><br>
						A dedicated Broader Impacts section is not a required component of the proposal paper.
					</li>
					<li><strong>I see an option for supplementary material in CMT - is it necessary to submit any additional material for this?</strong><br>
						By default, it is not assumed that supplementary material will be required for the proposal paper.  However, it is available as an option to cover unusual use-cases (e.g. if the authors wish to refer to a video, or other forms of media).
					</li>
					<li><strong>Where can I found more information about preregistration?</strong><br>
						There are a number of good resources for further reading around the ideas related to preregistration, including, but not limited to:
						<ul>
							<li><a href="https://www.pnas.org/content/115/11/2600">The preregistration revolution</a></li>
							<li><a href="https://www.nature.com/articles/s41562-016-0021">A manifesto for reproducible science</a></li>
							<li><a href="https://arxiv.org/abs/1904.10922">The Scientific Method in the Science of Machine Learning</a></li>
							<li><a href="https://cos.io/prereg/">The Center for Open Science</a> which includes a tool for preregistration and many more resources for further reading.
						</ul>
					</li>
				</ul>
			</div>
		</section>

		<section class="main style1 special">
			<div class="container">
				<header class="major">
					<h2>Organisers</h2>
				</header>
				<div class="row gtr-150">
					
					<div class="off-1 col-2 off-0-medium col-4-medium">
						<span class="image fit"><img src="images/luca.jpg" alt="Luca Bertinetto"></span>
						<h3><a href="https://www.robots.ox.ac.uk/~luca/">Luca Bertinetto</a></h3>
						<p>Five</p>
					</div>

					<div class="col-2 col-4-medium">
						<span class="image fit"><img src="images/joao.jpg" alt="Jo&atilde;o F. Henriques"></span>
						<h3><a href="https://www.robots.ox.ac.uk/~joao/">Jo&atilde;o F. Henriques</a></h3>
						<p>University of Oxford</p>
					</div>

					<div class="col-2 col-4-medium">
						<span class="image fit"><img src="images/samuel.png" alt="Samuel Albanie"></span>
						<h3><a href="https://www.robots.ox.ac.uk/~albanie/">Samuel Albanie</a></h3>
						<p>University of Oxford</p>
					</div>
					
					<div class="col-2 col-4-medium">
						<span class="image fit">
							<span class="image fit"><img src="images/michela.png" alt="Michela Paganini"></span>
						</span>
						<h3><a href="https://mickypaganini.github.io/">Michela Paganini</a></h3>
						<p>Facebook AI Research</p>
					</div>

					<div class="col-2 col-4-medium">
						<span class="image fit"><img src="images/gul.jpg" alt="G&uuml;l Varol"></span>
						<h3><a href="https://www.robots.ox.ac.uk/~gul/">G&uuml;l <br>Varol</a></h3>
						<p>University of Oxford</p>
					</div>
					
				</div>

                <header class="major">
                    <h2>Reviewers</h2>
                </header>
                <b>Many thanks to all the reviewers for their help:</b>
                <div class="row">
                    <div class="col-4 col-4-medium">
                        Minttu  Alakuijala<br>
                        Yuki    Asano<br>
                        Max Bain<br>
                        Fabien  Baradel<br>
                        Eloïse  Berthier<br>
                        Raphaël Berthier<br>
                        Alberto Bietti<br>
                        Hakan   Bilen<br>
                        Tolga   Birdal<br>
                        Oumayma Bounou<br>
                        Margaux Bregere<br>
                        Andrew  Brown<br>
                        Andrei  Bursuc<br>
                        Lénaïc  Chizat<br>
                        Jesse   Dodge<br>
                        Yuming  Du<br>
                        Christophe  Dupuy<br>
                        Sebastien   Ehrhardt<br>
                        Valentin    Gabeur<br>
                        Andrew  Gambardella<br>
                        Aude    Genevay<br>
                        Pascal  Germain<br>
                        Adam    Golinski<br>
                        Stuart  Golodetz<br>
                        Oliver  Groth<br>
                    </div>
                    <div class="col-4 col-4-medium">
                        Tom Gunter<br>
                        Kai Han<br>
                        Tengda  Han<br>
                        Yana    Hasson<br>
                        Eldar   Insafutdinov<br>
                        Ahmet   Iscen<br>
                        Xu  Ji<br>
                        Vicky   Kalogeiton<br>
                        A. Sophia   Koepke<br>
                        Viveka  Kulharia<br>
                        Valdimar Steinar Ericsson   Laenen<br>
                        Zihang  Lai<br>
                        Iro Laina<br>
                        Shuda   Li<br>
                        Roxane  Licandro<br>
                        Erika   Lu<br>
                        Robert  McCraith<br>
                        Eric    Metodiev<br>
                        Grégoire    Mialon<br>
                        Liliane Momeni<br>
                        Arsha   Nagrani<br>
                        Nantas  Nardelli<br>
                        Lukas   Neumann<br>
                        Maxime  Oquab<br>
                        Anuj    Pahuja<br>
                    </div>
                    <div class="col-4 col-4-medium">
                        Alexander Pashevich<br>
                        Mandela Patrick<br>
                        Loucas  Pillaud Vivien<br>
                        Ameya   Prabhu<br>
                        Tom Rainforth<br>
                        Ignacio Rocco<br>
                        Manon   Romain<br>
                        Vincent Roulet<br>
                        Christian   Rupprecht<br>
                        Levent  Sagun<br>
                        Lukas   Schäfer<br>
                        Li  Shen<br>
                        Oriane  Siméoni<br>
                        Umut    Simsekli<br>
                        Robin   Strudel<br>
                        Adrien  Taylor<br>
                        Damien  Teney<br>
                        James   Thornton<br>
                        Jack    Valmadre<br>
                        Bichen  Wu<br>
                        Shangzhe    Wu<br>
                        Weidi   Xie<br>
                        Charig  Yang<br>
                        Chuhan  Zhang<br>
                    </div>
                </div>

				<header class="major">
					<h2>Questions?</h2>
				</header>
				<ul class="actions special">
					<li><a href="mailto:preregistration2020@googlegroups.com" class="button wide primary">Contact us</a></li>
				</ul>
			</div>
		</section>

		<!-- Footer -->
		<section id="footer">
			<h3><strong>Share with friends who <span class="icon fa-heart-o"></span> science:</strong></h3>
		<ul class="icons">
		<li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fpreregister.science" target="_blank" class="icon alt fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fpreregister.science" target="_blank" class="icon alt fa-facebook"><span class="label">Facebook</span></a></li>
		<li><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fpreregister.science" target="_blank" class="icon alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="mailto:?subject=NeurIPS 2020 Workshop on Pre-registration&body=https%3A%2F%2Fpreregister.science" class="icon alt fa-envelope"><span class="label">Share by e-mail</span></a></li>
		</ul>
		<ul class="copyright">
		<li>&copy; Visual Geometry Group</li>
		<li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
		<li>Photo: <a href="https://unsplash.com/photos/I_LgQ8JZFGE?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank">Jo&atilde;o Silas</a></li>
		</ul>
		</section>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>
